# Trent Decaf Compiler AMOD-4901

This repo contains a simple programming language for Trent's AMOD-4901 course. The compiler is based off the expresso language from MIT's 2002 version of [SMA-5502](https://dspace.mit.edu/bitstream/handle/1721.1/35804/6-035Fall-2002/OcwWeb/Electrical-Engineering-and-Computer-Science/6-035Computer-Language-EngineeringFall2002/CourseHome/index.htm).

## Getting Started

### Requirements
Below is a list of required software dependencies that must be installed on your system in order to compile and run the project:
* .NET SDK 6 (required)
  * The entire compiler is written in cSharp and as such .NET is necessary in order to run and compile any project.
* [ANTLR Toolkit](https://www.antlr.org/download.html) (semi-required)
  * This is a parsing and lexing framework equivlant to JLEX and CUP, the tooling is required in order to generate the lexer and parser for building the compiler.
* Taskfile (semi-optional)
  * Taskfile is a general language agnostic task system that we use for running shell commands it let's us run simple shell commands such as `task` to build the entire compiler without having to use something like `make` or `.sh` files allowing for better cross platform use. If you do not have taskfile installed the compiler can be build by copying the commands directly and running them in bash, or the corresponding commands on whatever platform your using.

### Dependencies
* [CommandLineParser](https://github.com/commandlineparser/commandline)
  * This is a simple library for building a command line application it makes the options system nice and easy to use.
* [ANTLR](https://www.antlr.org/)
  * This is used for parsing and lexing.
* [Verify](https://github.com/VerifyTests/Verify)
  * This library is used for snapshot testing.

### Building
The compiler can be built using the command `task build`.

#### Usage

By default the compiler can be used by running `task -- <opts>`, you can run `task experiment` which will run the compiler with whatever is in `./example.decaf`

### Testing

The compiler uses `MSTest` and [`Verify`](https://github.com/VerifyTests/Verify) for testing. 

Tests can be run using `task test`, you can use `task test -- --filter <query>` to run specific tests.

### Extra Commands
* `task format` - This can be used to format the entire compiler and test suite.
* `task clean` - This can be used to clean up any artifacts generated by the compiler.


## Walkthrough

### CLI

TODO: Write documentation

### Lexing

Our language uses [antlr](https://www.antlr.org/) for lexing, we implement the lexing rules according to the `expresso_spec`. The grammar exists in [`./decaf/Frontend/DecafLexer.g4`](./decaf/Frontend/DecafLexer.g4).

The following tokens are implemented:

* Keywords (Meaningful words within the language)
  * boolean
  * callout
  * class
  * else
  * extends
  * false
  * if
  * int
  * new
  * null
  * return
  * this
  * true
  * void
  * while
* Attributes (Text items we skip over)
  * Whitespace/WSS (`space`,`\t`,`newline`)
  * Comments (start with `//` and end at end of line)
  * newline (`\n`,`\r\n`)
* Operators
  * General
    * LPAREN (`(`)
    * RPAREN (`)`)
    * LBRACE (`{`)
    * RBRACE (`}`)
    * LBRACK (`[`)
    * RBRACK (`]`)
    * SEMI (`;`)
    * COMMA (`,`)
    * DOT (`.`)
  * Prefix
    * NOT (`!`)
  * Arithmetic
    * PLUS (`+`)
    * MINUS (`-`)
    * MULT (`*`)
    * DIV (`/`)
  * Relational
    * LEQ (`<=`)
    * GEQ (`>=`)
    * GT (`>`)
    * LT (`<`)
  * Equality
    * EQ (`==`)
    * NEQ (`!=`)
  * Conditional
    * AND (`&&`)
    * OR (`||`)
  * Assignment
    * ASSIGN (`=`)
* Literals
  * INTLIT (e.g. `1234`)
  * BOOLLIT (`true`,`false`)
  * NULLLIT (`null`)
* Identifiers
  * ID (e.g. `myVariable123`)

The lexer will read the source file and convert the characters in the file into a stream of tokens that can be interpreted by the parser. This is referred to as a scanner based parser compared to a scannerless parser which would read characters directly from the source file. The advantage of using a scanner when combined with lalr parsing used by antlr is that the number of parser states is much lower you don't need one for every valid character instead you need one for every valid token in every valid state. We made the decision to handle literal values such as integers and booleans in the lexer rather than the parser as this also reduces the number of tokens and simplifies the parser at the small cost of slightly less contextual error messages.

Tests for the lexer can be found in [`./decafTests/LexerTests.cs`](./decafTests/LexerTests.cs).


### Parsing

We also use `Antlr` for parsing the grammar can be found in [`./decaf/Frontend/DecafParser.g4`](./decaf/Frontend/DecafParser.g4)

Antlr generates `DecafParser.cs` from this grammar which is programmatic implementation of an LL parser. In [`./decaf/Frontend/ParseTree.cs`](./decaf/Frontend/ParseTree.cs) we map the antlr rule contexts to a proper parsetree which we can use for furthur program analysis and to compile your program.

When constructing our parseTree we ignore a few rules such as `new` expressions as they won't be supported in our language.

Our tests for parsing are implemented in [`./decafTests/ParserTests.cs`](./decafTests/ParserTests.cs), we use snapshot testing and capture the output of the parseTree based on the input program.

### TODO: Semantic Analysis

### TODO: Code Generation


## TODO:
This section contains a list of general TODO's left on the project:
* Parsing
  * Create a ParseTreeVisitor file.
    * An abstract class where we can implement enter, and exit for each node and collect information, or return a value.
    * This will be useful for mapping the parseTree to a TypedTree.
    * This will be useful for semantic analysis.
  * Testing
    * Operator Precedence
    * Kitchen Soup
    * Targeted Tests

Copyright ©️ 2025 Jake Follest, Tony Tran