# Trent Decaf Compiler AMOD-4901

This repo contains a simple programming language for Trent's AMOD-4901 course. The compiler is based off the expresso language from MIT's 2002 version of [SMA-5502](https://dspace.mit.edu/bitstream/handle/1721.1/35804/6-035Fall-2002/OcwWeb/Electrical-Engineering-and-Computer-Science/6-035Computer-Language-EngineeringFall2002/CourseHome/index.htm).

## Getting Started

### Requirements
Below is a list of required software dependencies that must be installed on your system in order to compile and run the project:
* .NET SDK 6 (required)
  * The entire compiler is written in cSharp and as such .NET is necessary in order to run and compile any project.
* [ANTLR Toolkit](https://www.antlr.org/download.html) (semi-required)
  * This is a parsing and lexing framework equivlant to JLEX and CUP, the tooling is required in order to generate the lexer and parser for building the compiler.
* Taskfile (semi-optional)
  * Taskfile is a general language agnostic task system that we use for running shell commands it let's us run simple shell commands such as `task` to build the entire compiler without having to use something like `make` or `.sh` files allowing for better cross platform use. If you do not have taskfile installed the compiler can be build by copying the commands directly and running them in bash, or the corresponding commands on whatever platform your using.

### Dependencies
* [CommandLineParser](https://github.com/commandlineparser/commandline)
  * This is a simple library for building a command line application it makes the options system nice and easy to use.
* [ANTLR](https://www.antlr.org/)
  * This is used for parsing and lexing.
* [Verify](https://github.com/VerifyTests/Verify)
  * This library is used for snapshot testing.

### Building
The compiler can be built using the command `task build`.

#### Usage

By default the compiler can be used by running `task -- <opts>`, you can run `task experiment` which will run the compiler with whatever is in `./example.decaf`

### Testing

The compiler uses `MSTest` and [`Verify`](https://github.com/VerifyTests/Verify) for testing. 

Tests can be run using `task test`, you can use `task test -- --filter <query>` to run specific tests.

### Extra Commands
* `task format` - This can be used to format the entire compiler and test suite.
* `task clean` - This can be used to clean up any artifacts generated by the compiler.


## Walkthrough

### CLI

TODO: Write documentation

### Lexing

Our language uses [antlr](https://www.antlr.org/) for lexing, we implement the lexing rules according to the `expresso_spec`. The grammar exists in [`./decaf/Frontend/DecafLexer.g4`](./decaf/Frontend/DecafLexer.g4).

The following tokens are implemented:

* Keywords (Meaningful words within the language)
  * boolean
  * callout
  * class
  * else
  * extends
  * false
  * if
  * int
  * new
  * null
  * return
  * this
  * true
  * void
  * while
* Attributes (Text items we skip over)
  * Whitespace/WSS (`space`,`\t`,`newline`)
  * Comments (start with `//` and end at end of line)
  * newline (`\n`,`\r\n`)
* Operators
  * General
    * LPAREN (`(`)
    * RPAREN (`)`)
    * LBRACE (`{`)
    * RBRACE (`}`)
    * LBRACK (`[`)
    * RBRACK (`]`)
    * SEMI (`;`)
    * COMMA (`,`)
    * DOT (`.`)
  * Prefix
    * NOT (`!`)
  * Arithmetic
    * PLUS (`+`)
    * MINUS (`-`)
    * MULT (`*`)
    * DIV (`/`)
  * Relational
    * LEQ (`<=`)
    * GEQ (`>=`)
    * GT (`>`)
    * LT (`<`)
  * Equality
    * EQ (`==`)
    * NEQ (`!=`)
  * Conditional
    * AND (`&&`)
    * OR (`||`)
  * Assignment
    * ASSIGN (`=`)
* Literals
  * INTLIT (e.g. `1234`)
  * BOOLLIT (`true`,`false`)
  * NULLLIT (`null`)
* Identifiers
  * ID (e.g. `myVariable123`)

The lexer will read the source file and convert the characters in the file into a stream of tokens that can be interpreted by the parser. This is referred to as a scanner based parser compared to a scannerless parser which would read characters directly from the source file. The advantage of using a scanner when combined with lalr parsing used by antlr is that the number of parser states is much lower you don't need one for every valid character instead you need one for every valid token in every valid state. We made the decision to handle literal values such as integers and booleans in the lexer rather than the parser as this also reduces the number of tokens and simplifies the parser at the small cost of slightly less contextual error messages.

Tests for the lexer can be found in [`./decafTests/LexerTests.cs`](./decafTests/LexerTests.cs).


### WIP: Parsing

TODO: Write documentation

### TODO: Semantic Analysis

### TODO: Code Generation


## TODO:
This section contains a list of general TODO's left on the project:
* Parsing
  * Discuss grammar changes
    * There are a number of changes I think we should consider making to the grammar.
    * Some of them would allow us to parse more programs with the tradeoff that it's really easy to restrict things semantically later, parsing more stuff then less can greatly simplify our grammar while allowing for future generalization.
  * Consider if there is a simpler way of representing the ast.
    * It would be nice if there was a simpler way of representing our ast, adt's are great for this but c# doesn't have them i've used classes that inherit at the moment however this posses an issue in a case like `CallExpr` which can also be a statement we want to restrict the types for exhaustiveness however we may need a stop gap like an `ExpressionStatement` node which could contain an expression and just rely on the parser never generating anything else.
  * Create a ParseTreeVisitor file.
    * An abstract class where we can implement enter, and exit for each node and collect information, or return a value.
    * This will be useful for mapping the parseTree to a TypedTree.
    * This will be useful for semantic analysis.
  * Testing
    * Operator Precedence
    * Kitchen Soup
    * Targeted Tests

Copyright ©️ 2025 Jake Follest, Tony Tran